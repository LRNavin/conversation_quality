{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "\n",
    "# Model - Logistic Reg,\n",
    "# Features -\n",
    "#     * All Channels - Raw, Abs, Mag (8)\n",
    "#     * All Windows  - 1, 3, 5, 10, 15\n",
    "#     * All Indiv    - Stat - Mean, Variance, Spec - PSD 6 bins\n",
    "#     * All Pairwise -\n",
    "#            - Synch - Correl, lag-Correl, MI, mimicry\n",
    "#            - Convr - Sym.Conv, Asym.Conv, Glob.Conv\n",
    "#     * All GroupFeat-\n",
    "#            - Aggreagtion - Min, Max, Mean, Mode, Var\n",
    "#            -\n",
    "# Evaluation - Acc, Conf.Matrix, AUC, Precision, Recall,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/navinlr/Desktop/Thesis/code_base/conversation_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Groups = 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from modeling import dataset_provider as data_gen\n",
    "from feature_extract import turntake_extractor as tt_extractor\n",
    "import constants\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, mean_squared_error, roc_auc_score, r2_score, explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn import under_sampling \n",
    "from imblearn import over_sampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for baseline\n",
    "random_seed=44\n",
    "manifest=\"indiv\"\n",
    "data_split_per=.40\n",
    "missing_data_thresh=50.0 #(in percent)\n",
    "convq_thresh=3.0\n",
    "agreeability_thresh=.2\n",
    "annotators=[\"Divya\", \"Nakul\"]#, \"Swathi\"]\n",
    "only_involved_pairs=True\n",
    "splits = 5\n",
    "if manifest==\"group\":\n",
    "    smote_nn = 2\n",
    "else:\n",
    "    smote_nn = 6\n",
    "\n",
    "label_type = \"hard\"\n",
    "model_type = \"rand-for\"\n",
    "zero_mean  = False\n",
    "\n",
    "dataset=constants.features_dataset_path_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \n",
    "def over_sample_data(temp_X, temp_y, method=\"SMOTE\", k_neighbors=6):\n",
    "    if method == \"SMOTE\":\n",
    "        temp_X, temp_y = SMOTE(k_neighbors=k_neighbors-1, random_state=random_seed).fit_resample(temp_X, temp_y)\n",
    "    return temp_X, temp_y\n",
    "\n",
    "def feature_normalize(temp_X, method=\"min-max\"):\n",
    "    # Fit on training set only.\n",
    "    if method == \"min-max\":\n",
    "        normaliser = MinMaxScaler().fit(temp_X)\n",
    "    elif method == \"mean-var\":\n",
    "        normaliser = StandardScaler().fit(temp_X)\n",
    "    return normaliser\n",
    "    \n",
    "def feature_selection(temp_X, temp_y, method=\"anova\"):\n",
    "    top_features = []\n",
    "    if method == \"anova\":\n",
    "        f_values, p_values = f_classif(temp_X, temp_y)\n",
    "        top_features=np.where(np.array(p_values) <= 0.05)[0]\n",
    "#         print(top_features)\n",
    "        print(\"# Top Features = \" + str(len(top_features)))\n",
    "    return top_features\n",
    "\n",
    "def select_required_features(temp_X, required_feats):\n",
    "    temp_X=temp_X[:,required_feats]\n",
    "#     print(\"After Feature Selection, Features -> \" + str(temp_X.shape))\n",
    "    return temp_X\n",
    "\n",
    "def dimension_reduction(temp_X, method=\"pca\"):\n",
    "    dim_red_model = None\n",
    "    if method==\"pca\":\n",
    "        dim_red_model = PCA(.95).fit(temp_X)\n",
    "    elif method==\"tsne\":\n",
    "        dim_red_model = TSNE(n_components=2).fit(temp_X)\n",
    "    return dim_red_model\n",
    "    \n",
    "def process_convq_labels(y, label_type=\"soft\"):\n",
    "    print(\"Data-type of labels - \" + str(type(y)))\n",
    "    if label_type==\"soft\":\n",
    "        y=list(np.around(np.array(y),2))\n",
    "    else:\n",
    "        y=list(np.where(np.array(y) <= convq_thresh, 0, 1))\n",
    "        print(\"ConvQ Classes Distribution : (Total = \"+ str(len(y)) +\")\")\n",
    "        print(\"High Quality Conv = \" + str(sum(y)))\n",
    "        print(\"Low Quality Conv = \" + str(len(y)-sum(y)))\n",
    "    return y\n",
    "\n",
    "def model_convq_manifestation(temp_X, temp_y, model=\"log-reg\"):\n",
    "\n",
    "    if model == \"log-reg\":\n",
    "        model = LogisticRegression(solver='lbfgs', max_iter=1000, class_weight='balanced').fit(temp_X, temp_y)\n",
    "    elif model == \"lin-reg\":\n",
    "        model = LinearRegression().fit(temp_X, temp_y)\n",
    "    elif model == \"adaboost\":\n",
    "        model = AdaBoostClassifier(n_estimators=500).fit(temp_X, temp_y)\n",
    "    elif model == \"dec-tree\":\n",
    "        model = DecisionTreeClassifier(class_weight='balanced').fit(temp_X, temp_y)\n",
    "    elif model == \"rand-for\":\n",
    "        model = RandomForestClassifier(n_estimators=1000).fit(temp_X, temp_y)\n",
    "    elif model == \"svm\":\n",
    "        model = SVC(kernel='poly').fit(temp_X, temp_y)\n",
    "    elif model == \"log-elastic\":\n",
    "        model = SGDClassifier(loss=\"log\", max_iter=10000, class_weight='balanced').fit(temp_X, temp_y)\n",
    "    elif model == \"grad-boost-quant\":\n",
    "        model = GradientBoostingRegressor(loss='quantile',n_estimators=1000).fit(temp_X, temp_y)\n",
    "    return model\n",
    "\n",
    "def analyse_model_params(model):\n",
    "    return True\n",
    "\n",
    "def test_model(temp_X, model):\n",
    "    return model.predict(temp_X)\n",
    "\n",
    "def evaluate_predict(test_temp_y, predict_temp_y, method=accuracy_score):\n",
    "    score = method(test_temp_y, predict_temp_y)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dataset for modeling - indiv ConvQ, ...........\n",
      "Number of Groups (After removing missing data) - 85\n",
      "ZERO-MEAN Technique ? - False\n",
      "ZERO-MEAN Technique ? - False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Final Data-points (After removing unreliable annotation data) - 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179it [00:36,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Read\n",
    "X, y, ids = data_gen.get_dataset_for_experiment(dataset=dataset,\n",
    "                                                    manifest=manifest,\n",
    "                                                    missing_data_thresh=missing_data_thresh,\n",
    "                                                    agreeability_thresh=agreeability_thresh,\n",
    "                                                    annotators=annotators,\n",
    "                                                    only_involved_pairs=only_involved_pairs,\n",
    "                                                    zero_mean=zero_mean)\n",
    "\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statistics as s\n",
    "\n",
    "# print(s.mean(y))\n",
    "# print(s.stdev(y))\n",
    "# print(s.mode(y))\n",
    "# print(s.median(y))\n",
    "\n",
    "# top_20_ind = int(len(y)*.1)\n",
    "# print(top_20_ind)\n",
    "# temp_y = sorted(list(y))\n",
    "# print(temp_y[top_20_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-type of labels - <class 'list'>\n",
      "ConvQ Classes Distribution : (Total = 179)\n",
      "High Quality Conv = 163\n",
      "Low Quality Conv = 16\n"
     ]
    }
   ],
   "source": [
    "# Label Prep\n",
    "# Hard/Soft Labels\n",
    "y = process_convq_labels(y, label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data -> Features - (260, 79) and Labels - 260\n",
      "Test  Data -> Features - (36, 79) and Labels - 36\n",
      "130\n",
      "Train Data -> Features - (260, 83) and Labels - 260\n",
      "Test  Data -> Features - (36, 83) and Labels - 36\n",
      "130\n",
      "Train Data -> Features - (260, 81) and Labels - 260\n",
      "Test  Data -> Features - (36, 81) and Labels - 36\n",
      "130\n",
      "Train Data -> Features - (262, 82) and Labels - 262\n",
      "Test  Data -> Features - (36, 82) and Labels - 36\n",
      "131\n",
      "Train Data -> Features - (262, 81) and Labels - 262\n",
      "Test  Data -> Features - (35, 81) and Labels - 35\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "#train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=data_split_per, random_state=random_seed)\n",
    "final_conf_matrix = [[0,0],[0,0]]\n",
    "final_auc_score = 0.0\n",
    "final_r_squared = 0.0\n",
    "final_expl_vari = 0.0\n",
    "iterative_auc = 0.0\n",
    "final_predict = []\n",
    "final_pred_prob = []\n",
    "final_label = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=splits)\n",
    "for train_index, test_index in skf.split(final_X, y):\n",
    "\n",
    "    # Data Prep\n",
    "    train_X, test_X  = final_X[train_index], final_X[test_index]\n",
    "    train_y, test_y  = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "        \n",
    "    # Transform Features\n",
    "    normaliser = feature_normalize(train_X, method=\"mean-var\")\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    train_X = normaliser.transform(train_X)\n",
    "    test_X  = normaliser.transform(test_X)\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    dimension_model = dimension_reduction(train_X, method=\"pca\")\n",
    "    train_X = dimension_model.transform(train_X)\n",
    "    test_X  = dimension_model.transform(test_X)\n",
    "    \n",
    "    # Feature Selection\n",
    "    top_features = feature_selection(train_X, train_y, method=\"anova\")\n",
    "    train_X = select_required_features(train_X, top_features) \n",
    "    test_X  = select_required_features(test_X, top_features) \n",
    "\n",
    "    # SAMPLING\n",
    "    train_X, train_y = over_sample_data(train_X, train_y, method=\"SMOTE\", k_neighbors=smote_nn)\n",
    "    \n",
    "    print(\"Train Data -> Features - \" + str(train_X.shape) + \" and Labels - \" + str(len(train_y)))\n",
    "    print(\"Test  Data -> Features - \" + str(test_X.shape) + \" and Labels - \" + str(len(test_y)))\n",
    "    print(str(sum(train_y))) \n",
    "    \n",
    "    # Modelling\n",
    "    model         = model_convq_manifestation(train_X, train_y, model_type)\n",
    "    #Predict\n",
    "    predict_y     = test_model(test_X, model) \n",
    "    predict_proba = get_model_predict_proba(test_X, model)\n",
    "    \n",
    "    iterative_auc = iterative_auc + evaluate_predict(test_y, predict_y, roc_auc_score)\n",
    "    \n",
    "    final_predict.extend(predict_y)\n",
    "    final_pred_prob.extend(predict_proba)\n",
    "    final_label.extend(test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_conf_matrix = evaluate_predict(final_label, final_predict, confusion_matrix)\n",
    "final_auc_score   = evaluate_predict(final_label, final_predict, roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~ Confusion Matrix ~~~~~~~~~~~\n",
      "[[  4  11]\n",
      " [ 12 152]]\n",
      "~~~~~~~~~~~ AUC Score ~~~~~~~~~~~\n",
      "0.6253968656428089\n"
     ]
    }
   ],
   "source": [
    "print(\"~~~~~~~~~~~ Confusion Matrix ~~~~~~~~~~~\")\n",
    "print(final_conf_matrix)\n",
    "print(\"~~~~~~~~~~~ AUC Score ~~~~~~~~~~~\")\n",
    "print(final_auc_score)\n",
    "print(\"~~~~~~~~~~~ Iterative AUC ~~~~~~~~~~~~\")\n",
    "print(iterative_auc/skf.get_n_splits(final_X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(final_label, final_pred_prob)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(final_auc_score), color='b')\n",
    "plt.plot([0, 1], [0, 1], color='red', alpha=0.5, lw=2, linestyle='--')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
