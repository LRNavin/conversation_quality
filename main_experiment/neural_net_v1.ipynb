{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "\n",
    "# Model - Logistic Reg,\n",
    "# Features -\n",
    "#     * All Channels - Raw, Abs, Mag (8)\n",
    "#     * All Windows  - 1, 3, 5, 10, 15\n",
    "#     * All Indiv    - Stat - Mean, Variance, Spec - PSD 6 bins\n",
    "#     * All Pairwise -\n",
    "#            - Synch - Correl, lag-Correl, MI, mimicry\n",
    "#            - Convr - Sym.Conv, Asym.Conv, Glob.Conv\n",
    "#     * All GroupFeat-\n",
    "#            - Aggreagtion - Min, Max, Mean, Mode, Var\n",
    "#            -\n",
    "# Evaluation - Acc, Conf.Matrix, AUC, Precision, Recall,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/navinlr/Desktop/Thesis/code_base/conversation_quality')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Groups = 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from modeling import dataset_provider as data_gen\n",
    "import constants\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, mean_squared_error, roc_auc_score, r2_score, explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn import under_sampling \n",
    "from imblearn import over_sampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for baseline\n",
    "random_seed=20\n",
    "manifest=\"indiv\"\n",
    "data_split_per=.30\n",
    "missing_data_thresh=50.0 #(in percent)\n",
    "convq_thresh=3.0\n",
    "agreeability_thresh=.2\n",
    "annotators=[\"Divya\", \"Nakul\"]#, \"Swathi\"]\n",
    "only_involved_pairs=True\n",
    "splits = 5\n",
    "if manifest==\"group\":\n",
    "    smote_nn = 2\n",
    "else:\n",
    "    smote_nn = 6\n",
    "\n",
    "label_type = \"hard\"\n",
    "model_type = \"baseline_nn\"\n",
    "zero_mean  = False\n",
    "\n",
    "dataset=constants.features_dataset_path_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \n",
    "def over_sample_data(temp_X, temp_y, method=\"SMOTE\", k_neighbors=6):\n",
    "    if method == \"SMOTE\":\n",
    "        temp_X, temp_y = SMOTE(k_neighbors=k_neighbors-1).fit_resample(temp_X, temp_y)\n",
    "    return temp_X, temp_y\n",
    "\n",
    "def feature_normalize(temp_X, method=\"min-max\"):\n",
    "    # Fit on training set only.\n",
    "    if method == \"min-max\":\n",
    "        normaliser = MinMaxScaler().fit(temp_X)\n",
    "    elif method == \"mean-var\":\n",
    "        normaliser = StandardScaler().fit(temp_X)\n",
    "    return normaliser\n",
    "    \n",
    "def feature_selection(temp_X, temp_y, method=\"anova\"):\n",
    "    top_features = []\n",
    "    if method == \"anova\":\n",
    "        f_values, p_values = f_classif(temp_X, temp_y)\n",
    "        top_features=np.where(np.array(p_values) <= 0.05)[0]\n",
    "#         print(top_features)\n",
    "        print(\"# Top Features = \" + str(len(top_features)))\n",
    "    return top_features\n",
    "\n",
    "def select_required_features(temp_X, required_feats):\n",
    "    temp_X=temp_X[:,required_feats]\n",
    "#     print(\"After Feature Selection, Features -> \" + str(temp_X.shape))\n",
    "    return temp_X\n",
    "\n",
    "def dimension_reduction(temp_X, method=\"pca\"):\n",
    "    dim_red_model = None\n",
    "    if method==\"pca\":\n",
    "        dim_red_model = PCA(.95).fit(temp_X)\n",
    "    return dim_red_model\n",
    "    \n",
    "def process_convq_labels(y, label_type=\"soft\"):\n",
    "    print(\"Data-type of labels - \" + str(type(y)))\n",
    "    if label_type==\"soft\":\n",
    "        y=list(np.around(np.array(y),2))\n",
    "    else:\n",
    "        y=list(np.where(np.array(y) <= convq_thresh, 1, 0))\n",
    "        print(\"ConvQ Classes Distribution : (Total = \"+ str(len(y)) +\")\")\n",
    "        print(\"High Quality Conv = \" + str(sum(y)))\n",
    "        print(\"Low Quality Conv = \" + str(len(y)-sum(y)))\n",
    "    return y\n",
    "\n",
    "def model_convq_manifestation(temp_X, temp_y, model=\"log-reg\"):\n",
    "\n",
    "    if model == \"log-reg\":\n",
    "        model = LogisticRegression(solver='lbfgs', max_iter=1000).fit(temp_X, temp_y)\n",
    "    elif model == \"lin-reg\":\n",
    "        model = LinearRegression().fit(temp_X, temp_y)\n",
    "    elif model == \"adaboost\":\n",
    "        model = AdaBoostClassifier(n_estimators=100).fit(temp_X, temp_y)\n",
    "    elif model == \"baseline_nn\":\n",
    "        model = Baseline_NN(temp_X.shape[1])\n",
    "        print(model)\n",
    "    return model\n",
    "\n",
    "def analyse_model_params(model):\n",
    "    return True\n",
    "\n",
    "def test_model(temp_X, model):\n",
    "    return model.predict(temp_X)\n",
    "\n",
    "def evaluate_predict(predict_temp_y, test_temp_y, method=accuracy_score):\n",
    "    score = method(test_temp_y, predict_temp_y)\n",
    "    return score\n",
    "\n",
    "# baseline model\n",
    "class Baseline_NN(nn.Module):\n",
    "    # TODO:  Layers ip, op sizes common variable\n",
    "    def __init__(self, input_dim):\n",
    "        # ASsuming input dim (# features) is around > 10K\n",
    "        super(Baseline_NN, self).__init__()\n",
    "        print(\"Input Dimension is \" + str(input_dim))\n",
    "#         self.inp_lay = nn.Linear(input_dim, int(input_dim/10))\n",
    "        self.inp_lay = nn.Linear(input_dim, int(input_dim*2))\n",
    "        self.hiddn_1 = nn.Linear(int(input_dim*2), int(input_dim*2/10))\n",
    "        self.hiddn_2 = nn.Linear(int(input_dim*2/10), int(input_dim*2/100))\n",
    "        self.hiddn_3 = nn.Linear(int(input_dim*2/100), int(input_dim*2/1000))\n",
    "        self.out_lay = nn.Linear(int(input_dim*2/1000), 1) \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.batchnorm_il = nn.BatchNorm1d(int(input_dim*2))\n",
    "        self.batchnorm_h1 = nn.BatchNorm1d(int(input_dim*2/10))\n",
    "        self.batchnorm_h2 = nn.BatchNorm1d(int(input_dim*2/100))\n",
    "        self.batchnorm_h3 = nn.BatchNorm1d(int(input_dim*2/1000))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input Layer \n",
    "        x = self.relu(self.inp_lay(inputs))\n",
    "        x = self.batchnorm_il(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Hidden Layer 1\n",
    "        x = self.relu(self.hiddn_1(x))\n",
    "        x = self.batchnorm_h1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Hidden Layer 2\n",
    "        x = self.relu(self.hiddn_2(x))\n",
    "        x = self.batchnorm_h2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Hidden Layer 3\n",
    "        x = self.relu(self.hiddn_3(x))\n",
    "        x = self.batchnorm_h3(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.out_lay(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "## train data formatter\n",
    "class load_train_data(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "## test data formatter   \n",
    "class load_test_data(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "        \n",
    "def nn_dataloader(train_X, train_y, test_X, batch_size):\n",
    "    # Format Dataset\n",
    "    train_data = load_train_data(torch.FloatTensor(train_X), torch.FloatTensor(train_y))\n",
    "    test_data  = load_test_data(torch.FloatTensor(test_X))\n",
    "\n",
    "    #Convert formatted data to data loader\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(dataset=test_data, batch_size=1)\n",
    "    return train_loader, test_loader\n",
    "    \n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)    \n",
    "    return acc\n",
    "    \n",
    "def train_nn_model(model, train_loader, weight, num_epochs):\n",
    "    print(\"Model in Training Mode\")\n",
    "    model.train()\n",
    "    \n",
    "    # Class weights - Formula = Max(Class-Distburion)/Current_class-Distbution\n",
    "    class_weight = torch.FloatTensor(weight)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for e in tqdm(range(1, num_epochs+1)):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for data in train_loader:\n",
    "            X_batch, y_batch = data\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            acc  = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "    \n",
    "    return True\n",
    "\n",
    "def predict_with_nn(test_loader, model):\n",
    "    y_pred_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag  = torch.round(y_test_pred)\n",
    "            y_pred_list.append(y_pred_tag)\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    return y_pred_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dataset for modeling - indiv ConvQ, ...........\n",
      "Number of Groups (After removing missing data) - 85\n",
      "ZERO-MEAN Technique ? - False\n",
      "ZERO-MEAN Technique ? - False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Final Data-points (After removing unreliable annotation data) - 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179it [00:31,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data Read\n",
    "X, y, ids = data_gen.get_dataset_for_experiment(dataset=dataset,\n",
    "                                                    manifest=manifest,\n",
    "                                                    missing_data_thresh=missing_data_thresh,\n",
    "                                                    agreeability_thresh=agreeability_thresh,\n",
    "                                                    annotators=annotators,\n",
    "                                                    only_involved_pairs=only_involved_pairs,\n",
    "                                                    zero_mean=zero_mean)\n",
    "\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-type of labels - <class 'list'>\n",
      "ConvQ Classes Distribution : (Total = 179)\n",
      "High Quality Conv = 16\n",
      "Low Quality Conv = 163\n"
     ]
    }
   ],
   "source": [
    "# Label Prep\n",
    "# Hard/Soft Labels\n",
    "y = process_convq_labels(y, label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data -> Features - (125, 28560) and Labels - 125\n",
      "Test  Data -> Features - (54, 28560) and Labels - 54\n",
      "Number of Positive (1-LowConvq) Sample = 9\n",
      "Weights for Positive (1-LowConvq) Class = [11.1875]\n",
      "Input Dimension is 28560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline_NN(\n",
      "  (inp_lay): Linear(in_features=28560, out_features=57120, bias=True)\n",
      "  (hiddn_1): Linear(in_features=57120, out_features=5712, bias=True)\n",
      "  (hiddn_2): Linear(in_features=5712, out_features=571, bias=True)\n",
      "  (hiddn_3): Linear(in_features=571, out_features=57, bias=True)\n",
      "  (out_lay): Linear(in_features=57, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (batchnorm_il): BatchNorm1d(57120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_h1): BatchNorm1d(5712, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_h2): BatchNorm1d(571, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_h3): BatchNorm1d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Model in Training Mode\n"
     ]
    }
   ],
   "source": [
    "# Data Prep\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=data_split_per, random_state=random_seed)\n",
    "\n",
    "final_conf_matrix = [[0,0],[0,0]]\n",
    "final_auc_score = 0.0\n",
    "final_r_squared = 0.0\n",
    "final_expl_vari = 0.0\n",
    "\n",
    "# Neural Net variables\n",
    "batch_size=10\n",
    "num_epochs=100\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=splits)\n",
    "# for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "# # Data Prep\n",
    "# train_X, test_X  = X[train_index], X[test_index]\n",
    "# train_y, test_y  = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "# Transform Features\n",
    "normaliser = feature_normalize(train_X, method=\"mean-var\")\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_X = normaliser.transform(train_X)\n",
    "test_X  = normaliser.transform(test_X)\n",
    "\n",
    "# SAMPLING\n",
    "# train_X, train_y = over_sample_data(train_X, train_y, method=\"SMOTE\", k_neighbors=smote_nn)\n",
    "\n",
    "print(\"Train Data -> Features - \" + str(train_X.shape) + \" and Labels - \" + str(len(train_y)))\n",
    "print(\"Test  Data -> Features - \" + str(test_X.shape) + \" and Labels - \" + str(len(test_y)))\n",
    "print(\"Number of Positive (1-LowConvq) Sample = \" + str(sum(train_y))) \n",
    "\n",
    "# NN Data Loader\n",
    "train_loader, test_loader = nn_dataloader(train_X, train_y, test_X, batch_size)\n",
    "\n",
    "# Modelling\n",
    "\n",
    "weight=[len(y)/sum(y)] # Class Imbalance handler weights. \n",
    "print('Weights for Positive (1-LowConvq) Class = ' + str(weight))\n",
    "model = model_convq_manifestation(train_X, train_y, model_type)\n",
    "_ = train_nn_model(model, train_loader, weight, num_epochs)\n",
    "\n",
    "#Predict\n",
    "predict_y = predict_with_nn(test_loader, model)   \n",
    "\n",
    "# Evaluate\n",
    "conf_matrix = evaluate_predict(test_y, predict_y, confusion_matrix)\n",
    "try:\n",
    "    auc_score = evaluate_predict(test_y, predict_y, roc_auc_score)\n",
    "except ValueError:\n",
    "    auc_score = 0.0\n",
    "    print(\"Oops! All Predicitons in same class. Bad Fold... Fold not considered. for AUC\")\n",
    "#     r_squared = evaluate_predict(test_y, predict_y, r2_score)\n",
    "#     expl_vari = evaluate_predict(test_y, predict_y, explained_variance_score)\n",
    "\n",
    "print(\"Current Fold Prediciton Eval...\")\n",
    "print(conf_matrix)\n",
    "print(auc_score)\n",
    "\n",
    "#Update Cross Validated scores\n",
    "final_conf_matrix = final_conf_matrix + conf_matrix\n",
    "final_auc_score = final_auc_score + auc_score\n",
    "#     final_r_squared = final_r_squared + r_squared\n",
    "#     final_expl_vari = final_expl_vari + expl_vari\n",
    "\n",
    "# final_auc_score = final_auc_score/skf.get_n_splits(X, y)\n",
    "# final_r_squared = final_r_squared/skf.get_n_splits(X, y)\n",
    "# final_expl_vari = final_expl_vari/skf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~ Confusion Matrix ~~~~~~~~~~~\n",
      "[[26  5]\n",
      " [21  2]]\n",
      "~~~~~~~~~~~ AUC Score ~~~~~~~~~~~\n",
      "0.4628330995792427\n"
     ]
    }
   ],
   "source": [
    "# Printing Final Score\n",
    "# print(\"~~~~~~~~~~~ R^2 Measure ~~~~~~~~~~~\")\n",
    "# print(final_r_squared)\n",
    "# print(\"~~~~~~~~~~~ Explained Variance ~~~~~~~~~~~\")\n",
    "# print(final_expl_vari)\n",
    "print(\"~~~~~~~~~~~ Confusion Matrix ~~~~~~~~~~~\")\n",
    "print(final_conf_matrix)\n",
    "print(\"~~~~~~~~~~~ AUC Score ~~~~~~~~~~~\")\n",
    "print(final_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
