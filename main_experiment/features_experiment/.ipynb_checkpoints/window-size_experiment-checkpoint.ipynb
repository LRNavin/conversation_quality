{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Experiments - Window Size\n",
    "\n",
    "# Model - Logistic Reg, Log Loss + Elastic Regularisation + SGD\n",
    "# Features -\n",
    "#     * All Channels - Raw, Abs, Mag (8)\n",
    "#     * All Windows  - 0\n",
    "#     * All Indiv    - nil\n",
    "#     * All Pairwise -\n",
    "#            - Synch - Correl, lag-Correl, MI, mimicry, Coherence and Granger's\n",
    "#            - Convr - Sym.Conv, Asym.Conv, Glob.Conv\n",
    "#     * All GroupFeat-\n",
    "#            - Aggreagtion - Min, Max, Mean, Mode, Var\n",
    "#            \n",
    "# Evaluation - Acc, Conf.Matrix, AUC, Precision, Recall,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/navinlr/Desktop/Thesis/code_base/conversation_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Groups = 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navinlr/opt/anaconda3/envs/conversation_quality/lib/python3.7/site-packages/outdated/utils.py:18: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.3.4, the latest is 0.3.5.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  **kwargs\n",
      "/Users/navinlr/opt/anaconda3/envs/conversation_quality/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from modeling import dataset_provider as data_gen\n",
    "from feature_extract import turntake_extractor as tt_extractor\n",
    "import constants\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, mean_squared_error, roc_auc_score, r2_score, explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, RFE, chi2\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn import under_sampling \n",
    "from imblearn import over_sampling\n",
    "from sklearn.metrics import auc\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for baseline\n",
    "random_seed=44\n",
    "manifest=\"indiv\"\n",
    "data_split_per=.40\n",
    "missing_data_thresh=50.0 #(in percent)\n",
    "convq_thresh=3.0\n",
    "agreeability_thresh=.2\n",
    "annotators=[\"Divya\", \"Nakul\"]#, \"Swathi\"]\n",
    "only_involved_pairs=True\n",
    "use_tt_features=False\n",
    "splits = 5\n",
    "if manifest==\"group\":\n",
    "    smote_nn = 2\n",
    "else:\n",
    "    smote_nn = 6\n",
    "\n",
    "tt_features = [\"conv_eq\", \"#turns\", \"%talk\", \"mean_turn\", \"mean_silence\", \"%silence\", \"#bc\", \"%overlap\", \"#suc_interupt\", \"#un_interupt\"]\n",
    "\n",
    "label_type = \"hard\"\n",
    "model_type = \"log-elastic\"\n",
    "feat_sel = \"anova\"\n",
    "zero_mean  = False\n",
    "\n",
    "datasets=[constants.features_dataset_path_v7]#, constants.features_dataset_path_v13] #, constants.features_dataset_path_v13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \n",
    "def over_sample_data(temp_X, temp_y, method=\"SMOTE\", k_neighbors=6):\n",
    "    if method == \"SMOTE\":\n",
    "        temp_X, temp_y = SMOTE(k_neighbors=k_neighbors-1, random_state=random_seed).fit_resample(temp_X, temp_y)\n",
    "    return temp_X, temp_y\n",
    "\n",
    "def feature_normalize(temp_X, method=\"min-max\"):\n",
    "    # Fit on training set only.\n",
    "    if method == \"min-max\":\n",
    "        normaliser = MinMaxScaler().fit(temp_X)\n",
    "    elif method == \"mean-var\":\n",
    "        normaliser = StandardScaler().fit(temp_X)\n",
    "    return normaliser\n",
    "    \n",
    "def feature_selection(temp_X, temp_y, method=\"anova\", num_feats=None):\n",
    "    top_features = []\n",
    "    if method == \"anova\":\n",
    "        f_values, p_values = f_classif(temp_X, temp_y)\n",
    "        threshold=np.median(np.absolute(np.array(f_values)))*num_feats\n",
    "        top_features=np.where(np.absolute(np.array(f_values)) >= threshold)[0]\n",
    "#         top_features=np.where(np.absolute(np.array(p_values)) <= 0.5)[0]\n",
    "    elif method == \"tree-based\":\n",
    "        sel_model = SelectFromModel(RandomForestClassifier(n_estimators=100, class_weight='balanced'),\n",
    "                                    max_features=num_feats, threshold=-np.inf).fit(temp_X, temp_y)\n",
    "        top_features = sel_model.get_support(True)\n",
    "    elif method == \"rec-elim\":\n",
    "        sel_model = RFE(estimator=LogisticRegression(solver='lbfgs', max_iter=1000, class_weight='balanced'),\n",
    "                        n_features_to_select=num_feats, step=10000).fit(temp_X, temp_y)\n",
    "        top_features = sel_model.get_support(True)\n",
    "    elif method == \"lgbc\":\n",
    "        lgbc = LGBMClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "        sel_model = SelectFromModel(lgbc, max_features=num_feats, threshold=-np.inf).fit(temp_X, temp_y)\n",
    "        top_features = sel_model.get_support(True)\n",
    "\n",
    "    return top_features\n",
    "\n",
    "def select_required_features(temp_X, required_feats, method=\"anova\"):\n",
    "    if method == \"anova\":\n",
    "        temp_X=temp_X[:,required_feats]\n",
    "#     elif method\n",
    "    return temp_X\n",
    "\n",
    "def dimension_reduction(temp_X, method=\"pca\", threshold_var=0.95):\n",
    "    dim_red_model = None\n",
    "    if method==\"pca\":\n",
    "        dim_red_model = PCA(threshold_var).fit(temp_X)\n",
    "    elif method==\"tsne\":\n",
    "        dim_red_model = TSNE(n_components=2).fit(temp_X)\n",
    "    return dim_red_model\n",
    "    \n",
    "def process_convq_labels(y, label_type=\"soft\"):\n",
    "    print(\"Data-type of labels - \" + str(type(y)))\n",
    "    if label_type==\"soft\":\n",
    "        y=list(np.around(np.array(y),2))\n",
    "    else:\n",
    "        y=list(np.where(np.array(y) <= convq_thresh, 0, 1))\n",
    "        print(\"ConvQ Classes Distribution : (Total = \"+ str(len(y)) +\")\")\n",
    "        print(\"High Quality Conv = \" + str(sum(y)))\n",
    "        print(\"Low Quality Conv = \" + str(len(y)-sum(y)))\n",
    "    return y\n",
    "\n",
    "def model_convq_manifestation(temp_X, temp_y, model=\"log-reg\", fit=True):\n",
    "    \n",
    "    if fit:\n",
    "        if model == \"log-reg\":\n",
    "            model = LogisticRegression(solver='lbfgs', max_iter=1000, class_weight='balanced').fit(temp_X, temp_y)\n",
    "        elif model == \"lin-reg\":\n",
    "            model = LinearRegression().fit(temp_X, temp_y)\n",
    "        elif model == \"adaboost\":\n",
    "            model = AdaBoostClassifier(n_estimators=500).fit(temp_X, temp_y)\n",
    "        elif model == \"dec-tree\":\n",
    "            model = DecisionTreeClassifier(class_weight='balanced').fit(temp_X, temp_y)\n",
    "        elif model == \"rand-for\":\n",
    "            model = RandomForestClassifier(n_estimators=1000, class_weight='balanced').fit(temp_X, temp_y)\n",
    "        elif model == \"svm\":\n",
    "            model = SVC(kernel='poly').fit(temp_X, temp_y)\n",
    "        elif model == \"log-elastic\":\n",
    "            model = SGDClassifier(loss=\"log\", class_weight=\"balanced\", penalty=\"elasticnet\",\n",
    "                                  learning_rate='adaptive', eta0=0.001,\n",
    "                                  max_iter=10000).fit(temp_X, temp_y)\n",
    "        elif model == \"grad-boost-quant\":\n",
    "            model = GradientBoostingRegressor(loss='quantile',n_estimators=1000).fit(temp_X, temp_y)\n",
    "    else:\n",
    "        if model == \"log-reg\":\n",
    "            model = LogisticRegression(solver='lbfgs', max_iter=1000, class_weight='balanced')\n",
    "        elif model == \"lin-reg\":\n",
    "            model = LinearRegression()\n",
    "        elif model == \"adaboost\":\n",
    "            model = AdaBoostClassifier(n_estimators=500)\n",
    "        elif model == \"dec-tree\":\n",
    "            model = DecisionTreeClassifier(class_weight='balanced')\n",
    "        elif model == \"rand-for\":\n",
    "            model = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n",
    "        elif model == \"svm\":\n",
    "            model = SVC(kernel='poly')\n",
    "        elif model == \"log-elastic\":\n",
    "            model = SGDClassifier(loss=\"log\", max_iter=10000, class_weight='balanced')\n",
    "        elif model == \"grad-boost-quant\":\n",
    "            model = GradientBoostingRegressor(loss='quantile',n_estimators=1000)\n",
    "    return model\n",
    "\n",
    "def get_model_predict_proba(temp_X, model):\n",
    "    return model.predict_proba(temp_X)[::,1]\n",
    "\n",
    "def analyse_model_params(model):\n",
    "    return True\n",
    "\n",
    "def test_model(temp_X, model):\n",
    "    return model.predict(temp_X)\n",
    "\n",
    "def evaluate_predict(test_temp_y, predict_temp_y, method=accuracy_score):\n",
    "    score = method(test_temp_y, predict_temp_y)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dataset for modeling - indiv ConvQ, ...........\n",
      "Number of Groups (After removing missing data) - 85\n",
      "ZERO-MEAN Technique ? - False\n",
      "ZERO-MEAN Technique ? - False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 192.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Final Data-points (After removing unreliable annotation data) - 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179it [00:00, 200.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Feature Set - (179, 756)\n",
      "Data-type of labels - <class 'list'>\n",
      "ConvQ Classes Distribution : (Total = 179)\n",
      "High Quality Conv = 163\n",
      "Low Quality Conv = 16\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_feats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b240c9ddcc71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~~ !! @@ @@ @@ @@ Current Features Count = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_feats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" ~~ !! @@ @@ @@ @@\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mfinal_conf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_feats' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Read\n",
    "auc_experiment_dict={}\n",
    "\n",
    "\n",
    "for i, curr_dataset in enumerate(datasets):\n",
    "    X, y, ids = data_gen.get_dataset_for_experiment(dataset=curr_dataset,\n",
    "                                                        manifest=manifest,\n",
    "                                                        missing_data_thresh=missing_data_thresh,\n",
    "                                                        agreeability_thresh=agreeability_thresh,\n",
    "                                                        annotators=annotators,\n",
    "                                                        only_involved_pairs=only_involved_pairs,\n",
    "                                                        zero_mean=zero_mean)\n",
    "    print(\"Current Feature Set - \" + str(X.shape))\n",
    "    # Label Prep\n",
    "    # Hard/Soft Labels\n",
    "    y = process_convq_labels(y, label_type)\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"~~ !! @@ @@ @@ @@ Current Window = \" + str(i) + \" ~~ !! @@ @@ @@ @@\")\n",
    "    \n",
    "    final_conf_matrix = [[0,0],[0,0]]\n",
    "    final_auc_score = 0.0\n",
    "    final_r_squared = 0.0\n",
    "    final_expl_vari = 0.0\n",
    "    iterative_auc = 0.0\n",
    "    final_predict = []\n",
    "    final_pred_prob = []\n",
    "    final_label = []\n",
    "    cv_auc_scores = []\n",
    "    \n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(final_X, y)):\n",
    "\n",
    "        # Data Prep\n",
    "        train_X, test_X  = final_X[train_index], final_X[test_index]\n",
    "        train_y, test_y  = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "        # Transform Features\n",
    "        normaliser = feature_normalize(train_X, method=\"mean-var\")\n",
    "        # Apply transform to both the training set and the test set.\n",
    "        train_X = normaliser.transform(train_X)\n",
    "        test_X  = normaliser.transform(test_X)\n",
    "\n",
    "        # Dimensionality Reduction\n",
    "        dimension_model = dimension_reduction(train_X, method=\"pca\")\n",
    "        train_X = dimension_model.transform(train_X)\n",
    "        test_X  = dimension_model.transform(test_X)\n",
    "\n",
    "        print(\"After \" + \"PCA\" + \" Feature Set - \" + str(train_X.shape))\n",
    "\n",
    "        # Feature Selection\n",
    "        top_features = feature_selection(train_X, train_y, method=feat_sel, num_feats=3)\n",
    "        train_X = select_required_features(train_X, top_features) \n",
    "        test_X  = select_required_features(test_X, top_features) \n",
    "\n",
    "        print(\"After \" + feat_sel + \" Feature Set - \" + str(train_X.shape))\n",
    "\n",
    "\n",
    "        # SAMPLING\n",
    "        train_X, train_y = over_sample_data(train_X, train_y, method=\"SMOTE\", k_neighbors=smote_nn)\n",
    "\n",
    "        print(\"Train Data -> Features - \" + str(train_X.shape) + \" and Labels - \" + str(len(train_y)))\n",
    "        print(\"Test  Data -> Features - \" + str(test_X.shape) + \" and Labels - \" + str(len(test_y)))\n",
    "        print(str(sum(train_y))) \n",
    "\n",
    "        # Modelling\n",
    "        classifier         = model_convq_manifestation(train_X, train_y, model_type, fit=True)        \n",
    "        predict_y          = test_model(test_X, classifier) \n",
    "\n",
    "\n",
    "        final_conf_matrix = final_conf_matrix + evaluate_predict(test_y, predict_y, confusion_matrix)\n",
    "        \n",
    "        # CV ROC Plot\n",
    "        viz = plot_roc_curve(classifier, test_X, test_y,\n",
    "                         name='ROC fold {}'.format(i+1),\n",
    "                         alpha=0.3, lw=2, linestyle='--', color='b', ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='cornflowerblue',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=4, alpha=1.0)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.1,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "           title=\"Receiver operating characteristic example\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(aucs)\n",
    "    auc_experiment_dict[\"Window-\"+str(i)]=aucs\n",
    "    print(\"~~~~~~~~~~~ Confusion Matrix ~~~~~~~~~~~\")\n",
    "    print(final_conf_matrix)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
